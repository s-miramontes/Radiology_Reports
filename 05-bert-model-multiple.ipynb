{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbec6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch \n",
    "#print(torch.__version__)\n",
    "import transformers\n",
    "#print(transformers.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# importing some more dependencies\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from transformers.data.metrics import simple_accuracy\n",
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68680864",
   "metadata": {},
   "source": [
    "### Load tokenizer\n",
    "This will be the same tokenizer for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6631c53b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../model/UCSF BERT-500k+275k-pytorch/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da4e06",
   "metadata": {},
   "source": [
    "### Classification Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2b0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadNotes(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561d3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDocs(docList):\n",
    "    '''\n",
    "    Clean up labels and load as csv's.\n",
    "    Labels are changed to binary.\n",
    "    '''\n",
    "    dfs = []\n",
    "    binary_Map = {'Present': 1, 'Absent': 0}\n",
    "    for doc in docList:\n",
    "        df = pd.read_csv(doc)\n",
    "        df['label'] = [df['label'][j][2:-2] for j,i in enumerate(df['label'])]\n",
    "        df['label'] = df['label'].map(binary_Map)\n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb2f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFiles(organName, listDir, prefix=None):\n",
    "    \"\"\"\n",
    "    Generate list of files associa-\n",
    "    ted with each organ of interest\n",
    "    in the list of files.\n",
    "    --------------------------\n",
    "    Input:\n",
    "        organ_list: list of organs\n",
    "                    str\n",
    "        listDir:    list of files  \n",
    "                    to search\n",
    "        prefix:     OS path prefix\n",
    "    Output:\n",
    "        lists:      list of files \n",
    "                    associated with\n",
    "                    organ\n",
    "    \"\"\"\n",
    "    # get the files from listDir that contain 'organName'\n",
    "    matching = [file for file in listDir if organName in file]\n",
    "    \n",
    "    # append the path prefix so that files are accessible\n",
    "    if prefix:\n",
    "        matchingFixed = [prefix + file for file in matching]\n",
    "        return matchingFixed\n",
    "    \n",
    "    else:\n",
    "        return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3b9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, )\n",
    "    prec = precision_score(y_true=labels, y_pred=preds, )\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, )\n",
    "    macro_f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    macro_weighted_f1 = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    macro_precision = precision_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    macro_weighted_precision = precision_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    macro_recall = recall_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    macro_weighted_recall = recall_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    micro_f1 = f1_score(y_true=labels, y_pred=preds, average='micro')\n",
    "    confusion = confusion_matrix(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        'micro_f1': micro_f1,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"macro_weighted_f1\": macro_weighted_f1,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_weighted_precision\": macro_weighted_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_weighted_recall\": macro_weighted_recall,\n",
    "        \"confusion_matrix\": confusion,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c9daa",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795a9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data_200/recombined/'\n",
    "# should be the list of all files\n",
    "dataList = os.listdir(dataPath)\n",
    "dataList = [dataPath + file for file in dataList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce6c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do task splittings here (all abnormal findings, all disease location, all indeterminate nods, all prev surge)\n",
    "# model per classifier\n",
    "abnormal_files = getFiles('Abnormal Findings', dataList)\n",
    "abnormalDFs = cleanDocs(abnormal_files)\n",
    "# all in one big csv\n",
    "abnormalDFs = pd.concat(abnormalDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c224f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevsurgDFs = getFiles(\"Previous Surgeries\", dataList)\n",
    "prevsurgDFs = cleanDocs(prevsurgDFs)\n",
    "# al in one big csv\n",
    "prevsurgDFs = pd.concat(prevsurgDFs)\n",
    "\n",
    "diseaselocDFs = getFiles(\"disease_location\", dataList)\n",
    "diseaselocDFs = cleanDocs(diseaselocDFs)\n",
    "# all in one big csv\n",
    "diseaselocDFs = pd.concat(diseaselocDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5156c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Previous Surgeries Classifier\", \"Disease Location Classifier\"]\n",
    "all_ = [abnormalDFs, prevsurgDFs, diseaselocDFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89e43536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5fddb3bdeaf0903792b63904</td>\n",
       "      <td>Gastrostomy tube in place. No evidence of co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6046c8dfc7274cd918d2870b</td>\n",
       "      <td>Extensive multifocal areas of bowel wall thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e46a9a85808eee774f693f0</td>\n",
       "      <td>Interval placement of duodenal stent in the d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f5b7067d8d9aa5228644e8e</td>\n",
       "      <td>Unremarkable    \\r\\n Vasculature:  Portal ve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e4659115808eee77438cd61</td>\n",
       "      <td>Unremarkable    Pelvis:  Unremarkable    \\r\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>5e45fdc15808eee7744b92a0</td>\n",
       "      <td>For chest findings, please see the  separatel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>5e45eb425808eee774049345</td>\n",
       "      <td>For chest findings, please see the separatel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5e4674955808eee774ea5d4e</td>\n",
       "      <td>Right upper lobe subpleural noncalcified pul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>5e4606e15808eee7748c7160</td>\n",
       "      <td>For chest findings, please see the separatel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>5fde1c97eaf0903792741c50</td>\n",
       "      <td>For chest findings, please see the  separatel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          idx  \\\n",
       "0    5fddb3bdeaf0903792b63904   \n",
       "1    6046c8dfc7274cd918d2870b   \n",
       "2    5e46a9a85808eee774f693f0   \n",
       "3    5f5b7067d8d9aa5228644e8e   \n",
       "4    5e4659115808eee77438cd61   \n",
       "..                        ...   \n",
       "195  5e45fdc15808eee7744b92a0   \n",
       "196  5e45eb425808eee774049345   \n",
       "197  5e4674955808eee774ea5d4e   \n",
       "198  5e4606e15808eee7748c7160   \n",
       "199  5fde1c97eaf0903792741c50   \n",
       "\n",
       "                                              sentence  label  \n",
       "0      Gastrostomy tube in place. No evidence of co...      0  \n",
       "1      Extensive multifocal areas of bowel wall thi...      0  \n",
       "2     Interval placement of duodenal stent in the d...      0  \n",
       "3      Unremarkable    \\r\\n Vasculature:  Portal ve...      0  \n",
       "4       Unremarkable    Pelvis:  Unremarkable    \\r\\n       0  \n",
       "..                                                 ...    ...  \n",
       "195   For chest findings, please see the  separatel...      0  \n",
       "196    For chest findings, please see the separatel...      0  \n",
       "197    Right upper lobe subpleural noncalcified pul...      1  \n",
       "198    For chest findings, please see the separatel...      0  \n",
       "199   For chest findings, please see the  separatel...      0  \n",
       "\n",
       "[2200 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormalDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d44bc",
   "metadata": {},
   "source": [
    "### Training\n",
    "- Splits should happen here\n",
    "- How many models to generate?\n",
    "- How to save the models?\n",
    "    - Don't save everything, that may take a lot of space.\n",
    "- Compute metrics so that you can assess the best performing models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be44b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying training parameters\n",
    "batch_size = 32\n",
    "n_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5de76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 different test sets, actual labels, predicted labels\n",
    "test_sets = []\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "\n",
    "# list of dictionaries of each model (should be 3)\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357c2fd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model... Previous Surgeries Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/UCSF BERT-500k+275k-pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/UCSF BERT-500k+275k-pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/silviamiramontes/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training Previous Surgeries Classifier\n",
      "Now evaluating...\n",
      "Done! ... saving...\n",
      "Training model... Disease Location Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silviamiramontes/opt/anaconda3/lib/python3.8/site-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Some weights of the model checkpoint at ../model/UCSF BERT-500k+275k-pytorch/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/UCSF BERT-500k+275k-pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/silviamiramontes/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training Disease Location Classifier\n",
      "Now evaluating...\n",
      "Done! ... saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silviamiramontes/opt/anaconda3/lib/python3.8/site-packages/transformers/data/metrics/__init__.py:36: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# data splitting: all organs, one task?\n",
    "\n",
    "for ind, frame in enumerate(all_[1:]):\n",
    "    \n",
    "    print(\"Training model...\", names[ind])\n",
    "\n",
    "    X = frame['sentence']\n",
    "    y = frame['label']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    # determine class weights\n",
    "    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "    weight = 1. / class_sample_count\n",
    "    samples_weight = np.array([weight[t] for t in y_train])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    samples_weigth = samples_weight.double()\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    # encoding texts with tokenizer\n",
    "    train_encodings = tokenizer(X_train.tolist(), max_length=512, truncation=True, padding=True)\n",
    "    val_encodings = tokenizer(X_test.tolist(), max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # create dataset from texts in dfs\n",
    "    train_dataset = RadNotes(train_encodings, y_train.tolist())\n",
    "    val_dataset = RadNotes(val_encodings, y_test.tolist())\n",
    "    \n",
    "    # training parameters are specified above\n",
    "\n",
    "    # select appropriate device \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # now for model -- load\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('../model/UCSF BERT-500k+275k-pytorch/') \n",
    "    model.to(device) # model moved to available device\n",
    "    model.train() # set to training mode\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              sampler=sampler)\n",
    "    num_steps = len(train_loader) // n_epochs #total training in batch div by num of epochs\n",
    "\n",
    "    # optimization\n",
    "    optim =AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "                    optim, num_warmup_steps = warmup_steps, num_training_steps = num_steps\n",
    "                 )\n",
    "\n",
    "    # ---------------------------TRAINING------------------------------------\n",
    "    for epoch in range(n_epochs): # for each available epoch\n",
    "        for batch in train_loader: # for b in train_loader\n",
    "            optim.zero_grad() # gradient @ zero?\n",
    "            input_ids = batch['input_ids'].to(device) # keeping track of ids in device\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            # save loss from the outputs of the model\n",
    "            loss=outputs[0]\n",
    "\n",
    "            # now backprop\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "            scheduler.step() # updating learning rate schedule \n",
    "            model.zero_grad()\n",
    "    print(\"Finished training\", names[ind])\n",
    "    print(\"Now evaluating...\")\n",
    "\n",
    "    # ---------------------------EVALUATE MODEL------------------------------\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    preds, all_labels = None, None\n",
    "\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask = attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            all_labels = batch['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            all_labels = np.append(all_labels, batch['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    print(\"Done! ... saving...\")\n",
    "\n",
    "    # make dataframe for all outputs (preds, labels, sentences)\n",
    "    # save each model metric in a pandas dataframe (a list of dictionaries, which you later conver to df)\n",
    "    # note that you should specify the model that worked best.\n",
    "    # in the long run we should save the best model. \n",
    "\n",
    "    # result per csv split file\n",
    "    result = multiclass_acc_and_f1(preds, all_labels)\n",
    "    \n",
    "    # model comparison\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # result compiling\n",
    "    test_sets.append(X_test)\n",
    "    actual_labels.append(all_labels)\n",
    "    predicted_labels.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67ea187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'acc': 0.7549909255898367,\n",
       "  'f1': 0.34146341463414637,\n",
       "  'precision': 0.20833333333333334,\n",
       "  'recall': 0.9459459459459459,\n",
       "  'micro_f1': 0.7549909255898368,\n",
       "  'macro_f1': 0.5954808711966718,\n",
       "  'macro_weighted_f1': 0.8153834606346476,\n",
       "  'macro_precision': 0.6015557006092254,\n",
       "  'macro_weighted_precision': 0.9419678044034187,\n",
       "  'macro_recall': 0.8435955410663583,\n",
       "  'macro_weighted_recall': 0.7549909255898367,\n",
       "  'confusion_matrix': array([[381, 133],\n",
       "         [  2,  35]])},\n",
       " {'acc': 0.8690909090909091,\n",
       "  'f1': 0.5909090909090909,\n",
       "  'precision': 0.45217391304347826,\n",
       "  'recall': 0.8524590163934426,\n",
       "  'micro_f1': 0.8690909090909091,\n",
       "  'macro_f1': 0.7564935064935066,\n",
       "  'macro_weighted_f1': 0.8853482880755609,\n",
       "  'macro_precision': 0.7157421289355322,\n",
       "  'macro_weighted_precision': 0.9208461223933487,\n",
       "  'macro_recall': 0.8618123302826108,\n",
       "  'macro_weighted_recall': 0.8690909090909091,\n",
       "  'confusion_matrix': array([[426,  63],\n",
       "         [  9,  52]])}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45e824d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_0616 = pd.DataFrame(all_results, index=names)\n",
    "results_0616.to_csv(\"Classifier results_prevsurg_diseaseloc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7f169d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_weighted_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_weighted_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_weighted_recall</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Previous Surgeries Classifier</th>\n",
       "      <td>0.754991</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.754991</td>\n",
       "      <td>0.595481</td>\n",
       "      <td>0.815383</td>\n",
       "      <td>0.601556</td>\n",
       "      <td>0.941968</td>\n",
       "      <td>0.843596</td>\n",
       "      <td>0.754991</td>\n",
       "      <td>[[381, 133], [2, 35]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disease Location Classifier</th>\n",
       "      <td>0.869091</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.869091</td>\n",
       "      <td>0.756494</td>\n",
       "      <td>0.885348</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>0.920846</td>\n",
       "      <td>0.861812</td>\n",
       "      <td>0.869091</td>\n",
       "      <td>[[426, 63], [9, 52]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    acc        f1  precision    recall  \\\n",
       "Previous Surgeries Classifier  0.754991  0.341463   0.208333  0.945946   \n",
       "Disease Location Classifier    0.869091  0.590909   0.452174  0.852459   \n",
       "\n",
       "                               micro_f1  macro_f1  macro_weighted_f1  \\\n",
       "Previous Surgeries Classifier  0.754991  0.595481           0.815383   \n",
       "Disease Location Classifier    0.869091  0.756494           0.885348   \n",
       "\n",
       "                               macro_precision  macro_weighted_precision  \\\n",
       "Previous Surgeries Classifier         0.601556                  0.941968   \n",
       "Disease Location Classifier           0.715742                  0.920846   \n",
       "\n",
       "                               macro_recall  macro_weighted_recall  \\\n",
       "Previous Surgeries Classifier      0.843596               0.754991   \n",
       "Disease Location Classifier        0.861812               0.869091   \n",
       "\n",
       "                                    confusion_matrix  \n",
       "Previous Surgeries Classifier  [[381, 133], [2, 35]]  \n",
       "Disease Location Classifier     [[426, 63], [9, 52]]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_0616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b86a5105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>micro_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_weighted_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_weighted_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>macro_weighted_recall</th>\n",
       "      <th>confusion_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abnormal Findings Classifier</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>0.573991</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>0.732834</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.804172</td>\n",
       "      <td>0.820415</td>\n",
       "      <td>0.705401</td>\n",
       "      <td>0.827273</td>\n",
       "      <td>[[391  19]\\n [ 76  64]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Previous Surgeries Classifier</td>\n",
       "      <td>0.932849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932849</td>\n",
       "      <td>0.482629</td>\n",
       "      <td>0.900441</td>\n",
       "      <td>0.466425</td>\n",
       "      <td>0.870208</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.932849</td>\n",
       "      <td>[[514   0]\\n [ 37   0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disease Location Classifier</td>\n",
       "      <td>0.889091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.889091</td>\n",
       "      <td>0.470645</td>\n",
       "      <td>0.836892</td>\n",
       "      <td>0.444545</td>\n",
       "      <td>0.790483</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.889091</td>\n",
       "      <td>[[489   0]\\n [ 61   0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Unnamed: 0       acc        f1  precision    recall  \\\n",
       "0   Abnormal Findings Classifier  0.827273  0.573991   0.771084  0.457143   \n",
       "1  Previous Surgeries Classifier  0.932849  0.000000   0.000000  0.000000   \n",
       "2    Disease Location Classifier  0.889091  0.000000   0.000000  0.000000   \n",
       "\n",
       "   micro_f1  macro_f1  macro_weighted_f1  macro_precision  \\\n",
       "0  0.827273  0.732834           0.810811         0.804172   \n",
       "1  0.932849  0.482629           0.900441         0.466425   \n",
       "2  0.889091  0.470645           0.836892         0.444545   \n",
       "\n",
       "   macro_weighted_precision  macro_recall  macro_weighted_recall  \\\n",
       "0                  0.820415      0.705401               0.827273   \n",
       "1                  0.870208      0.500000               0.932849   \n",
       "2                  0.790483      0.500000               0.889091   \n",
       "\n",
       "          confusion_matrix  \n",
       "0  [[391  19]\\n [ 76  64]]  \n",
       "1  [[514   0]\\n [ 37   0]]  \n",
       "2  [[489   0]\\n [ 61   0]]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(\"Classifier results.csv\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d64db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving metrics of all classifiers\n",
    "df_res = pd.DataFrame(all_results, index=names)\n",
    "os.mkdir(\"Results\")\n",
    "df_res.to_csv(\"Classifier results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b5b15",
   "metadata": {},
   "source": [
    "Saving texts trained, predicted labels and actua labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "abfinds_res = pd.DataFrame(columns=['Text', 'Predicted', 'Actual'])\n",
    "abfinds_res['Text'] = test_sets[0]\n",
    "abfinds_res['Predicted'] = predicted_labels[0]\n",
    "abfinds_res['Actual'] = actual_labels[0]\n",
    "abfinds_res.to_csv('Abnormal_Findings_Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4a4524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_res = pd.DataFrame(columns=['Text', 'Predicted', 'Actual'])\n",
    "prev_res['Text'] = test_sets[1]\n",
    "prev_res['Predicted'] = predicted_labels[1]\n",
    "prev_res['Actual'] = actual_labels[1]\n",
    "prev_res.to_csv('Previous_surgeries_Results_weighted.csv')\n",
    "\n",
    "disease_res = pd.DataFrame(columns=['Text', 'Predicted', 'Actual'])\n",
    "disease_res['Text'] = test_sets[2]\n",
    "disease_res['Predicted'] = predicted_labels[2]\n",
    "disease_res['Actual'] = actual_labels[2]\n",
    "disease_res.to_csv('Disease_location_Results_weighted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6b8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
