{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b3f704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "4.20.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import torch \n",
    "print(torch.__version__)\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c24e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c00023",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a303a949",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# specify where the model docs are, note that you added \"model_type\":\"bert\" in line 1 of json file\n",
    "tokenizer = AutoTokenizer.from_pretrained('../model/UCSF BERT-500k+275k-pytorch/', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638a5b4",
   "metadata": {},
   "source": [
    "## Parragraph/doc level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332dd88",
   "metadata": {},
   "source": [
    "1. Preparing classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7402b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadNotes(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84441492",
   "metadata": {},
   "source": [
    "1.1 Get train, dev and test set texts and labels. Best way to do this is to load through csv or dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9b52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '../data_200/recombined/'\n",
    "dataList = os.listdir(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce07166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e4619235808eee774f5728d</td>\n",
       "      <td>Distended stomach and proximal duodenum, sim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ebd8129c206f20a8b300bfe</td>\n",
       "      <td>***** of the enteric tube terminates within ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5fddef39eaf0903792b27112</td>\n",
       "      <td>Unremarkable  Pelvis: Unremarkable  \\r\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5f5aaa30d8d9aa5228b83083</td>\n",
       "      <td>No bowel obstruction. Small fat-containing u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e463dc95808eee774ab89b0</td>\n",
       "      <td>Colonic diverticulosis.  Pelvis: Unremarkable...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        idx  \\\n",
       "0  5e4619235808eee774f5728d   \n",
       "1  5ebd8129c206f20a8b300bfe   \n",
       "2  5fddef39eaf0903792b27112   \n",
       "3  5f5aaa30d8d9aa5228b83083   \n",
       "4  5e463dc95808eee774ab89b0   \n",
       "\n",
       "                                            sentence  label  \n",
       "0    Distended stomach and proximal duodenum, sim...      0  \n",
       "1    ***** of the enteric tube terminates within ...      0  \n",
       "2          Unremarkable  Pelvis: Unremarkable  \\r\\n       0  \n",
       "3    No bowel obstruction. Small fat-containing u...      0  \n",
       "4   Colonic diverticulosis.  Pelvis: Unremarkable...      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abFinds_liver = pd.read_csv(dataPath + dataList[0])\n",
    "abFinds_pancreas = pd.read_csv(dataPath + dataList[2])\n",
    "# fix labels\n",
    "abFinds_liver['label'] = [abFinds_liver['label'][j][2:-2]for j,i in enumerate(abFinds_liver['label'])]\n",
    "abFinds_pancreas['label'] = [abFinds_pancreas['label'][j][2:-2]for j,i in enumerate(abFinds_pancreas['label'])]\n",
    "\n",
    "# map to ints\n",
    "binaryMap = {'Present': 1, 'Absent': 0}\n",
    "abFinds_liver['label'] = abFinds_liver['label'].map(binaryMap)\n",
    "abFinds_pancreas['label'] = abFinds_pancreas['label'].map(binaryMap)\n",
    "\n",
    "# concatenate abnormal findings for pancreas + liver\n",
    "abnormal_findings = pd.concat([abFinds_liver, abFinds_pancreas])\n",
    "abnormal_findings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e970fb4",
   "metadata": {},
   "source": [
    "2. Trying the train_test_split method from sklearn with a pandas dataframe passed in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cea249",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_abfinds = abnormal_findings['sentence']\n",
    "y_abfinds = abnormal_findings['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_abfinds, y_abfinds, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4b872c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Present, Absent) -- Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42, 258)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"(Present, Absent) -- Training\")\n",
    "sum(y_train[y_train == 1]), len(y_train[y_train != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93f45441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Present, Absent) -- Testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 90)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"(Present, Absent) -- Testing\")\n",
    "sum(y_test[y_test == 1]), len(y_test[y_test != 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a25192",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- Important to show this as a key step in presentation.\n",
    "- A good idea would be to show how the regeneration of the splits changes the performance of the algorithm.\n",
    "- Keep for now liver and pancreas for all 5 applications\n",
    "- Analyze all performances and compare. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99297ae6",
   "metadata": {},
   "source": [
    "2.1 Adding weights to each class to introduce low dist. penalization.\n",
    "Resources used: \n",
    "- https://towardsdatascience.com/address-class-imbalance-easily-with-pytorch-e2d4fa208627\n",
    "- https://discuss.pytorch.org/t/how-does-weightedrandomsampler-work/8089\n",
    "- https://www.scottcondron.com/jupyter/visualisation/audio/2020/12/02/dataloaders-samplers-collate.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13eaa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels : [0 1]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# class weighting\n",
    "labels_unique, counts = np.unique(abnormal_findings['label'], return_counts = True)\n",
    "print(\"Unique labels : {}\".format(labels_unique))\n",
    "class_weights = [sum(counts) / c for c in counts] # [#{class_0}, #{class_1}]\n",
    "\n",
    "# assign weight each input sample\n",
    "example_weights = [class_weights[e] for e in abnormal_findings['label']]\n",
    "sampler = WeightedRandomSampler(example_weights, len(abnormal_findings['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfb50b",
   "metadata": {},
   "source": [
    "3. Using tokenizer for the data we just split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8652d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding texts with the tokenizer\n",
    "train_encodings = tokenizer(X_train.tolist(), max_length = 512, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_test.tolist(), max_length=512, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02464c7",
   "metadata": {},
   "source": [
    "4. Create a dataset from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2335ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RadNotes(train_encodings, y_train.tolist())\n",
    "val_dataset = RadNotes(val_encodings, y_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b319b",
   "metadata": {},
   "source": [
    "5. Now specify training parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c79e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 0 # what is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176f610",
   "metadata": {},
   "source": [
    "6. Now the training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ed9ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../model/UCSF BERT-500k+275k-pytorch/ were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/UCSF BERT-500k+275k-pytorch/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/silviamiramontes/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4790f7c0f95d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# iterate for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for each available epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for b in train_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradient @ zero?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# keeping track of ids in device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-22750713a6dd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-22750713a6dd>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# importing some more dependencies\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# now choosing GPU if available, if not choosing CPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# now for the model---- load\n",
    "model = AutoModelForSequenceClassification.from_pretrained('../model/UCSF BERT-500k+275k-pytorch/') \n",
    "model.to(device) # model moved to available device\n",
    "model.train() # set to training mode\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=sampler, batch_size=batch_size)\n",
    "num_steps = len(train_loader) // n_epochs #total training in batch div by num of epochs\n",
    "\n",
    "# optimization\n",
    "optim =AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "                optim, num_warmup_steps = warmup_steps, num_training_steps = num_steps\n",
    "            )\n",
    "\n",
    "# iterate for training\n",
    "for epoch in range(n_epochs): # for each available epoch\n",
    "    for batch in train_loader: # for b in train_loader\n",
    "        optim.zero_grad() # gradient @ zero?\n",
    "        input_ids = batch['input_ids'].to(device) # keeping track of ids in device\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        # save loss from the outputs of the model\n",
    "        loss=outputs[0]\n",
    "        \n",
    "        # now backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        scheduler.step() # updating learning rate schedule \n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc85371",
   "metadata": {},
   "source": [
    "7. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a157ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "model.eval()\n",
    "\n",
    "preds, all_labels = None, None\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask = attention_mask)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        all_labels = batch['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        all_labels = np.append(all_labels, batch['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "preds = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623021a",
   "metadata": {},
   "source": [
    "8. Computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute evaluation metrics\n",
    "# Define desirable evaluation metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from transformers.data.metrics import simple_accuracy\n",
    "\n",
    "def multiclass_acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, )\n",
    "    prec = precision_score(y_true=labels, y_pred=preds, )\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, )\n",
    "    macro_f1 = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    macro_weighted_f1 = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    macro_precision = precision_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    macro_weighted_precision = precision_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    macro_recall = recall_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    macro_weighted_recall = recall_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    micro_f1 = f1_score(y_true=labels, y_pred=preds, average='micro')\n",
    "    confusion = confusion_matrix(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": recall,\n",
    "        'micro_f1': micro_f1,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"macro_weighted_f1\": macro_weighted_f1,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_weighted_precision\": macro_weighted_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_weighted_recall\": macro_weighted_recall,\n",
    "        \"confusion_matrix\": confusion,\n",
    "    }\n",
    "\n",
    "result = multiclass_acc_and_f1(preds, all_labels)\n",
    "\n",
    "print(\"Result: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3645b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(result, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b97d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d9181",
   "metadata": {},
   "source": [
    "1. Testable hypothesis: combining classifiers, does that make them improve or worsen? (separate for each classifier)\n",
    "    - One model per organ, and compare by combining all classifiers \n",
    "    - good to test\n",
    "    - increased power will outweigh fact of us having less specificity\n",
    "2. Focus on Previous surgeries, abnormal findings, and disease_location\n",
    "3. Once we have a classifier\n",
    "    - LIT or tensorboard\n",
    "    - trying to understand the word enrichment in one group vs another\n",
    "        - f1 scores do not inform us much\n",
    "    - send him the list afterwards for his insight\n",
    "        - test outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbd1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
